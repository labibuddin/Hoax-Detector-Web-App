{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "import csv\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "URL = 'https://cekfakta.com/page/'\n",
    "\n",
    "judul_lst = []\n",
    "desc_lst = []\n",
    "status_lst = []\n",
    "link_lst = []\n",
    "\n",
    "for page in range(1,1801):\n",
    "    \n",
    "\treq = requests.get(URL + str(page) + '/', verify=False)\n",
    "\tsoup = bs(req.text, 'html.parser')\n",
    "\n",
    "\tjudul_tag = soup.select('h1[class=\"title\"] a')\n",
    "\tfor i in judul_tag:\n",
    "\t\tjudul_lst.append(i.getText().replace('[SALAH]', ''))\n",
    "\n",
    "\t# desc_tag = soup.select('div[class=\"description\"] div[class=\"content\"]')\n",
    "\t# for i in desc_tag:\n",
    "\t# \tdesc_lst.append(i.getText())\n",
    "\t\n",
    "\tstatus_tag = soup.select('div[class=\"status\"] a')\n",
    "\tfor i in status_tag:\n",
    "\t\tstatus_lst.append(i.getText())\n",
    "  \n",
    "\tfor i in judul_tag:\n",
    "\t\tn = i['href']\n",
    "\t\tlink_lst.append(n)\t\n",
    "\t\n",
    "\tsleep(randint(2,10))\n",
    "\n",
    "dic_data = {\n",
    "    'Judul': judul_lst,\n",
    "    # 'Deskripsi': desc_lst,\n",
    "    'Status': status_lst,\n",
    "    'Link': link_lst\n",
    "}\n",
    "\n",
    "print(len(judul_lst))\n",
    "# print(len(desc_lst))\n",
    "print(len(status_lst))\n",
    "print(len(link_lst))\n",
    "\n",
    "# with open(\"trial.csv\", \"w\") as outfile:\n",
    "\n",
    "#     writer = csv.writer(outfile)\n",
    "#     writer.writerow(dic_data.keys())\n",
    "#     writer.writerows(zip(*dic_data.values()))\n",
    "\n",
    "# with open('trial.csv', newline='') as in_file:\n",
    "#     with open('output_trial.csv', 'w', newline='') as out_file:\n",
    "#         writer = csv.writer(out_file)\n",
    "#         for row in csv.reader(in_file):\n",
    "#             if any(row):\n",
    "#                 writer.writerow(row)\n",
    "\n",
    "# file = 'trial.csv'\n",
    "# if(os.path.exists(file) and os.path.isfile(file)):\n",
    "#     os.remove(file)\n",
    "\n",
    "# desc_List 564 kosong dan 1176 kosong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396\n",
      "1396\n",
      "1396\n",
      "1396\n"
     ]
    }
   ],
   "source": [
    "# print(judul_lst[1177])\n",
    "# # desc_lst.insert(1177, 'Deskripsi Kosong')\n",
    "# print(desc_lst[1177])\n",
    "# print(link_lst[1177])\n",
    "print(len(judul_lst))\n",
    "print(len(desc_lst))\n",
    "print(len(status_lst))\n",
    "print(len(link_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dic_data) \n",
    "df.to_csv (r'dataset_1396_5-25.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judul_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cekfakta.com/'\n",
    "page = requests.get(url, verify=False)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "judul_tag = soup.select('h1[class=\"title\"] a')\n",
    "judul_lst = [x.getText().replace('[SALAH]', '') for x in judul_tag]\n",
    "\n",
    "desc_tag = soup.select('div[class=\"description\"] div[class=\"content\"]')\n",
    "desc_lst = [x.getText() for x in desc_tag]\n",
    "\n",
    "status_tag = soup.select('div[class=\"status\"] a')\n",
    "status_lst = [x.getText() for x in status_tag]\n",
    "\n",
    "link_lst = []\n",
    "for i in judul_tag:\n",
    "    n = i['href']\n",
    "    link_lst.append(n)\n",
    "\n",
    "dic_data = {\n",
    "    'Judul': judul_lst,\n",
    "    'Deskripsi': desc_lst,\n",
    "    'Status': status_lst,\n",
    "    'Link': link_lst\n",
    "}\n",
    "\n",
    "with open(\"trial.csv\", \"w\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(dic_data.keys())\n",
    "    writer.writerows(zip(*dic_data.values()))\n",
    "\n",
    "with open('trial.csv', newline='') as in_file:\n",
    "    with open('output_trial.csv', 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        for row in csv.reader(in_file):\n",
    "            if any(row):\n",
    "                writer.writerow(row)\n",
    "\n",
    "file = 'trial.csv'\n",
    "if(os.path.exists(file) and os.path.isfile(file)):\n",
    "    os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cekfakta.com/benar/'\n",
    "page = requests.get(url, verify=False)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "desc_tag = soup.select('div[class=\"content\"]')\n",
    "desc_lst = [x.getText() for x in desc_tag]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "0\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "URL = 'https://cekfakta.com/benar/page/'\n",
    "\n",
    "judul_lst_benar = []\n",
    "desc_lst_benar = []\n",
    "status_lst_benar = []\n",
    "link_lst_benar = []\n",
    "\n",
    "for page in range(1,54):\n",
    "    \n",
    "\treq = requests.get(URL + str(page) + '/', verify=False)\n",
    "\tsoup = bs(req.text, 'html.parser')\n",
    "\n",
    "\tjudul_tag = soup.select('h1[class=\"title\"] a')\n",
    "\tfor i in judul_tag:\n",
    "\t\tjudul_lst_benar.append(i.getText())\n",
    "\n",
    "\t# desc_tag = soup.select('li[class=\"card\"] div[class=\"wrapper\"] div[class=\"content\"]')\n",
    "\t# for i in desc_tag:\n",
    "\t# \tdesc_lst_benar.append(i.getText())\n",
    "\t\n",
    "\tstatus_tag = soup.select('div[class=\"status\"] a')\n",
    "\tfor i in status_tag:\n",
    "\t\tstatus_lst_benar.append(i.getText())\n",
    "  \n",
    "\tfor i in judul_tag:\n",
    "\t\tn = i['href']\n",
    "\t\tlink_lst_benar.append(n)\t\n",
    "\t\n",
    "\tsleep(randint(2,10))\n",
    "\n",
    "# dic_data_benar = {\n",
    "#     'Judul': judul_lst_benar,\n",
    "#     'Deskripsi': desc_lst_benar,\n",
    "#     'Status': status_lst_benar,\n",
    "#     'Link': link_lst_benar\n",
    "# }\n",
    "\n",
    "# print(judul_lst_benar)\n",
    "print(len(judul_lst_benar))\n",
    "# print(desc_lst_benar)\n",
    "print(len(desc_lst_benar))\n",
    "# print(len(status_lst_benar))\n",
    "# print(link_lst_benar)\n",
    "print(len(link_lst_benar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "status_lst_benar = status_lst_benar[:211]\n",
    "print(len(status_lst_benar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n",
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(judul_lst_benar))\n",
    "print(len(desc_lst_benar))\n",
    "print(len(status_lst_benar))\n",
    "print(len(link_lst_benar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_data_benar = {\n",
    "    'Judul': judul_lst_benar,\n",
    "    # 'Deskripsi': desc_lst_benar,\n",
    "    'Status': status_lst_benar,\n",
    "    'Link': link_lst_benar\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dic_data_benar) \n",
    "df.to_csv (r'dataset_benarv2.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3af395df47b0f769527003f8933dfa19a4bece7db9e65e18852214d2513fa12"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
